{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "#from value_iteration import value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.042463633444655341,\n",
       " -0.04429724686096817,\n",
       " -0.027718077495514293,\n",
       " -0.019245419118063503)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "#action = env.action_space.sample()\n",
    "#observation, reward, done, info = env.step(action)\n",
    "tuple(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Zufällige Actions ausführen (ohne Terminalzustand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(80):\n",
    "    env.render()\n",
    "    done = None\n",
    "    for x in range(2):\n",
    "        done = env.step(0)\n",
    "        # take a random action\n",
    "    env.render()\n",
    "    time.sleep(0.2)\n",
    "    for x in range(2):\n",
    "        env.step(1) # take a random action\n",
    "    time.sleep(0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Zufällige Actions ausführen (mit Terminalzustand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 13 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 44 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Optimale Policy lernen mit dem DP Algorithmus \"Value Iteration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP Algorithmen können nicht eingesetzt werden, da die P Matrix (aka Model) aus dem MDP nicht verfügbar ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: Optimale Policy lernen mit On-Policy Monte Carlo Algorithmus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from MC_on_policy_control import mc_control_epsilon_greedy, sample\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7000/7000."
     ]
    }
   ],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=7000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 100 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        state = tuple(round(i,1) for i in state)\n",
    "        probabilities = policy(state)\n",
    "        totals = list(itertools.accumulate(probabilities))\n",
    "        action = sample(totals)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        if t == 99:\n",
    "            print(\"Episode finished after 100 timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
