{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "#from value_iteration import value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.042463633444655341,\n",
       " -0.04429724686096817,\n",
       " -0.027718077495514293,\n",
       " -0.019245419118063503)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "#action = env.action_space.sample()\n",
    "#observation, reward, done, info = env.step(action)\n",
    "tuple(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Zufällige Actions ausführen (mit Terminalzustand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 13 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 44 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Optimale Policy lernen mit On-Policy Monte Carlo Algorithmus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from MC_on_policy_control import mc_control_epsilon_greedy, sample\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7000/7000."
     ]
    }
   ],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=7000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 100 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        state = tuple(round(i,1) for i in state)\n",
    "        probabilities = policy(state)\n",
    "        totals = list(itertools.accumulate(probabilities))\n",
    "        action = sample(totals)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        if t == 99:\n",
    "            print(\"Episode finished after 100 timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Demo 3: Optimale Policy lernen mit On-Policy Monte Carlo Algorithmus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from MC_off_policy_control import mc_control_importance_sampling, sample\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=7000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        #print(observation)\n",
    "        state = tuple(round(i,1) for i in state)\n",
    "        probabilities = policy(state)\n",
    "        totals = list(itertools.accumulate(probabilities))\n",
    "        action = sample(totals)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        if t == 99:\n",
    "            print(\"Episode finished after 100 timesteps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
